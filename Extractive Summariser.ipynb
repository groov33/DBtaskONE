{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo videoID:\n",
    "# aDG1T0kJnd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to fetch transcript through API so:\n",
    "def FetchSubs(videoID):\n",
    "    try:\n",
    "        transcript=YouTubeTranscriptApi.get_transcript(videoID)\n",
    "        text=\" \".join(entry['text'] for entry in transcript) \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript fetched successfully!\n",
      "Here's the starting bit of the transcript: [Music] the Constitution of India is the supreme law of the land the longest written constitution of ...\n"
     ]
    }
   ],
   "source": [
    "videoID=input(\"Enter Video ID: \")\n",
    "transcript=FetchSubs(f\"{videoID}\")\n",
    "\n",
    "if transcript:\n",
    "    print(\"Transcript fetched successfully!\")\n",
    "print(f\"Here's the starting bit of the transcript: {transcript[:100]} ...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed tools\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "\n",
    "# Removing Square Brackets and Extra Spaces\n",
    "transcript = re.sub(r'\\[[0-9]*\\]', ' ', transcript)\n",
    "transcript = re.sub(r'\\s+', ' ', transcript)\n",
    "\n",
    "# Removing special characters and digits\n",
    "formatted_transcript = re.sub('[^a-zA-Z]', ' ', transcript )\n",
    "formatted_transcript = re.sub(r'\\s+', ' ', formatted_transcript)\n",
    "\n",
    "#Defining Stopwords\n",
    "stopwords= nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising to sentence and words\n",
    "sentence_list=nltk.sent_tokenize(transcript)\n",
    "word_list=nltk.word_tokenize(transcript)\n",
    "\n",
    "#Initialising word and sentence scores\n",
    "word_frequencies={}\n",
    "sentence_scores={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting word count:\n",
    "for word in word_list:\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word]=1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising word frequencies\n",
    "maximum_frequency=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]=(word_frequencies[word]/maximum_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining Sentence Scores\n",
    "#What i am doing here is- \n",
    "#Taking a sentence-> Breaking it into words-> Adding up word freq to determine importance of the sentence-> Later picking up few most important sentences.\n",
    "for sentence in sentence_list:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sentence.split(' '))<30:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence]=word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Fetching and joining most important sentences\n",
    "import heapq\n",
    "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
